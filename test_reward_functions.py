"""
Test script for the improved reward functions.

This script tests the improved reward functions with the actual Open-R1-Math-220k dataset.
"""

from datasets import load_dataset
import re
import math

def extract_answer(text):
    """
    Extract the answer from generated text.
    
    Args:
        text: The text generated by the model
        
    Returns:
        The extracted answer
    """
    # Check for "The answer is:" format
    answer_pattern = r'The answer is:?\s*(.*?)(?:\n|$)'
    match = re.search(answer_pattern, text, re.IGNORECASE)
    if match:
        return match.group(1).strip()
    
    # Check if the text already contains a formatted answer like v_{R}=4
    var_pattern = r'v_\{[^}]+\}\s*=\s*\d+'
    if re.search(var_pattern, text):
        return text.strip()
    
    # Try to extract from LaTeX format (common in math problems)
    latex_patterns = [
        r'=\s*(\d+(?:\.\d+)?)',  # Match = number (possibly with decimal)
        r'(\d+(?:\.\d+)?)\s*$'   # Match number at the end of text
    ]
    
    for pattern in latex_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    
    # Try to extract from XML tags if present
    match = re.search(r"<answer>\s*(.*?)\s*</answer>", text, re.DOTALL)
    if match:
        return match.group(1).strip()
    
    # If no pattern matches, return the last line as a fallback
    lines = text.strip().split('\n')
    return lines[-1].strip()

def normalize_answer(answer):
    """
    Normalize an answer for comparison.
    
    Args:
        answer: The answer to normalize
        
    Returns:
        The normalized answer
    """
    # For simple numeric answers
    if re.match(r'^\s*\d+(?:\.\d+)?\s*$', answer):
        return answer.strip()
    
    # For complex answers with variables, extract all numbers
    numbers = re.findall(r'=\s*(\d+(?:\.\d+)?)', answer)
    if numbers:
        return ','.join(numbers)
    
    # Remove LaTeX formatting as a fallback
    answer = re.sub(r'\\mathrm\{[^}]*\}', '', answer)
    answer = re.sub(r'\\', '', answer)
    answer = re.sub(r'\{|\}', '', answer)
    
    return answer.strip()

def compare_answers(generated, reference):
    """
    Compare a generated answer with a reference answer.
    
    Args:
        generated: The answer generated by the model
        reference: The reference answer
        
    Returns:
        True if the answers match, False otherwise
    """
    generated_norm = normalize_answer(generated)
    reference_norm = normalize_answer(reference)
    
    # If both contain multiple numbers (like v_R=4, v_B=10)
    gen_numbers = re.findall(r'\d+(?:\.\d+)?', generated_norm)
    ref_numbers = re.findall(r'\d+(?:\.\d+)?', reference_norm)
    
    if len(gen_numbers) > 1 and len(ref_numbers) > 1:
        # Check if all numbers match (order-independent)
        return sorted(gen_numbers) == sorted(ref_numbers)
    
    # Try numeric comparison for single numbers
    try:
        gen_float = float(generated_norm)
        ref_float = float(reference_norm)
        return math.isclose(gen_float, ref_float, rel_tol=1e-8)
    except (ValueError, TypeError):
        # Fall back to string comparison
        return generated_norm == reference_norm

def correctness_reward(generated_texts, reference_answers):
    """
    Calculate correctness rewards for generated answers.
    
    Args:
        generated_texts: List of texts generated by the model
        reference_answers: List of reference answers
        
    Returns:
        List of reward scores (1 for correct, 0 for incorrect)
    """
    rewards = []
    for gen_text, ref_answer in zip(generated_texts, reference_answers):
        extracted_answer = extract_answer(gen_text)
        if compare_answers(extracted_answer, ref_answer):
            rewards.append(1.0)
        else:
            rewards.append(0.0)
    return rewards

def step_by_step_reward(generated_texts, reference_solutions):
    """
    Calculate rewards for step-by-step reasoning.
    
    Args:
        generated_texts: List of texts generated by the model
        reference_solutions: List of reference solutions
        
    Returns:
        List of reward scores (0-1 based on similarity to reference solution)
    """
    rewards = []
    for gen_text, ref_solution in zip(generated_texts, reference_solutions):
        # Count the number of steps (lines) in the solution
        gen_steps = len([line for line in gen_text.split('\n') if line.strip()])
        ref_steps = len([line for line in ref_solution.split('\n') if line.strip()])
        
        # Calculate a simple ratio of steps
        step_ratio = min(gen_steps / max(1, ref_steps), 1.0)
        
        # Check if key equations are present
        key_equations = re.findall(r'\\begin\{array\}.*?\\end\{array\}', ref_solution, re.DOTALL)
        equation_score = 0.0
        if key_equations:
            for eq in key_equations:
                if eq in gen_text:
                    equation_score += 1.0 / len(key_equations)
        else:
            # If no equations found, look for mathematical expressions
            math_expressions = re.findall(r'\$.*?\$', ref_solution)
            if math_expressions:
                for expr in math_expressions:
                    if expr in gen_text:
                        equation_score += 1.0 / len(math_expressions)
            else:
                # If no math expressions found, give full score for this part
                equation_score = 1.0
        
        # Combine scores (50% for steps, 50% for equations)
        combined_score = 0.5 * step_ratio + 0.5 * equation_score
        rewards.append(combined_score)
    
    return rewards

def main():
    """Main function to test the reward functions."""
    print("=" * 50)
    print("Testing Improved Reward Functions with Open-R1-Math-220k Dataset")
    print("=" * 50)
    
    # Load a few examples from the dataset
    print("\nLoading dataset...")
    ds = load_dataset("open-r1/OpenR1-Math-220k")
    examples = ds['train'].select(range(5))  # Get 5 examples
    
    print("\nTesting reward functions on actual dataset examples:")
    for i, example in enumerate(examples):
        print(f"\nExample {i+1}:")
        print(f"Problem: {example['problem'][:100]}...")  # Show first 100 chars
        print(f"Answer: {example['answer']}")
        
        # Test with the actual solution
        solution = example['solution']
        
        # Test direct comparison of the answer field
        correct_reward = correctness_reward([example['answer']], [example['answer']])[0]
        print(f"Self-comparison of answer field: {correct_reward}")
        
        # Test with a simulated generated solution + answer
        simulated_output = solution + "\nThe answer is: " + example['answer']
        
        # Test answer extraction from simulated output
        extracted = extract_answer(simulated_output)
        normalized_extracted = normalize_answer(extracted)
        normalized_answer = normalize_answer(example['answer'])
        
        print(f"Extracted from simulated output: {extracted}")
        print(f"Normalized extracted: {normalized_extracted}")
        print(f"Normalized reference: {normalized_answer}")
        print(f"Extraction correct: {compare_answers(extracted, example['answer'])}")
        
        # Test correctness reward with simulated output
        correct_reward = correctness_reward([simulated_output], [example['answer']])[0]
        print(f"Correctness reward with simulated output: {correct_reward}")
        
        # Test step-by-step reward
        step_reward = step_by_step_reward([solution], [solution])[0]
        print(f"Step-by-step reward (self-comparison): {step_reward}")
        
        # Test with partial solution
        partial_solution = "\n".join(solution.split("\n")[:3])  # First 3 lines
        partial_reward = step_by_step_reward([partial_solution], [solution])[0]
        print(f"Step-by-step reward (partial solution): {partial_reward}")
    
    print("\nAll tests completed!")

if __name__ == "__main__":
    main() 