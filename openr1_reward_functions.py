"""
Improved reward functions for the Open-R1-Math-220k dataset.

This module contains improved reward functions for evaluating model outputs on the Open-R1-Math-220k dataset.
"""

import re
import math
from typing import List, Union, Dict, Any

def extract_answer(text: str) -> str:
    """
    Extract the answer from generated text.
    
    Args:
        text: The text generated by the model
        
    Returns:
        The extracted answer
    """
    # Check if the text already contains a formatted answer like v_{R}=4
    var_pattern = r'v_\{[^}]+\}\s*=\s*\d+'
    if re.search(var_pattern, text):
        return text.strip()
    
    # Try to extract from LaTeX format (common in math problems)
    latex_patterns = [
        r'=\s*(\d+(?:\.\d+)?)',  # Match = number (possibly with decimal)
        r'(\d+(?:\.\d+)?)\s*$'   # Match number at the end of text
    ]
    
    for pattern in latex_patterns:
        match = re.search(pattern, text)
        if match:
            return match.group(1).strip()
    
    # If no pattern matches, return the last line as a fallback
    lines = text.strip().split('\n')
    return lines[-1].strip()

def normalize_answer(answer: str) -> str:
    """
    Normalize an answer for comparison.
    
    Args:
        answer: The answer to normalize
        
    Returns:
        The normalized answer
    """
    # For simple numeric answers
    if re.match(r'^\s*\d+(?:\.\d+)?\s*$', answer):
        return answer.strip()
    
    # For complex answers with variables, extract all numbers
    numbers = re.findall(r'=\s*(\d+(?:\.\d+)?)', answer)
    if numbers:
        return ','.join(numbers)
    
    # Remove LaTeX formatting as a fallback
    answer = re.sub(r'\\mathrm\{[^}]*\}', '', answer)
    answer = re.sub(r'\\', '', answer)
    answer = re.sub(r'\{|\}', '', answer)
    
    return answer.strip()

def compare_answers(generated: str, reference: str) -> bool:
    """
    Compare a generated answer with a reference answer.
    
    Args:
        generated: The answer generated by the model
        reference: The reference answer
        
    Returns:
        True if the answers match, False otherwise
    """
    generated_norm = normalize_answer(generated)
    reference_norm = normalize_answer(reference)
    
    # If both contain multiple numbers (like v_R=4, v_B=10)
    gen_numbers = re.findall(r'\d+(?:\.\d+)?', generated_norm)
    ref_numbers = re.findall(r'\d+(?:\.\d+)?', reference_norm)
    
    if len(gen_numbers) > 1 and len(ref_numbers) > 1:
        # Check if all numbers match (order-independent)
        return sorted(gen_numbers) == sorted(ref_numbers)
    
    # Try numeric comparison for single numbers
    try:
        gen_float = float(generated_norm)
        ref_float = float(reference_norm)
        return math.isclose(gen_float, ref_float, rel_tol=1e-8)
    except (ValueError, TypeError):
        # Fall back to string comparison
        return generated_norm == reference_norm

def correctness_reward(generated_texts: List[str], reference_answers: List[str]) -> List[float]:
    """
    Calculate correctness rewards for generated answers.
    
    Args:
        generated_texts: List of texts generated by the model
        reference_answers: List of reference answers
        
    Returns:
        List of reward scores (1 for correct, 0 for incorrect)
    """
    rewards = []
    for gen_text, ref_answer in zip(generated_texts, reference_answers):
        extracted_answer = extract_answer(gen_text)
        if compare_answers(extracted_answer, ref_answer):
            rewards.append(1.0)
        else:
            rewards.append(0.0)
    return rewards

def step_by_step_reward(generated_texts: List[str], reference_solutions: List[str]) -> List[float]:
    """
    Calculate rewards for step-by-step reasoning.
    
    Args:
        generated_texts: List of texts generated by the model
        reference_solutions: List of reference solutions
        
    Returns:
        List of reward scores (0-1 based on similarity to reference solution)
    """
    rewards = []
    for gen_text, ref_solution in zip(generated_texts, reference_solutions):
        # Count the number of steps (lines) in the solution
        gen_steps = len([line for line in gen_text.split('\n') if line.strip()])
        ref_steps = len([line for line in ref_solution.split('\n') if line.strip()])
        
        # Calculate a simple ratio of steps
        step_ratio = min(gen_steps / max(1, ref_steps), 1.0)
        
        # Check if key equations are present
        key_equations = re.findall(r'\\begin\{array\}.*?\\end\{array\}', ref_solution, re.DOTALL)
        equation_score = 0.0
        if key_equations:
            for eq in key_equations:
                if eq in gen_text:
                    equation_score += 1.0 / len(key_equations)
        else:
            # If no equations found, look for mathematical expressions
            math_expressions = re.findall(r'\$.*?\$', ref_solution)
            if math_expressions:
                for expr in math_expressions:
                    if expr in gen_text:
                        equation_score += 1.0 / len(math_expressions)
            else:
                # If no math expressions found, give full score for this part
                equation_score = 1.0
        
        # Combine scores (50% for steps, 50% for equations)
        combined_score = 0.5 * step_ratio + 0.5 * equation_score
        rewards.append(combined_score)
    
    return rewards

def test_reward_functions():
    """Test the reward functions on sample outputs."""
    from datasets import load_dataset
    
    # Load a few examples from the dataset
    ds = load_dataset("open-r1/OpenR1-Math-220k")
    examples = ds['train'].select(range(3))
    
    print("\nTesting reward functions:")
    for i, example in enumerate(examples):
        print(f"\nExample {i+1}:")
        print(f"Problem: {example['problem'][:100]}...")  # Show first 100 chars
        print(f"Answer: {example['answer']}")
        
        # Test with correct answer
        correct_output = f"The answer is {example['answer']}"
        correct_reward = correctness_reward([correct_output], [example['answer']])[0]
        print(f"Reward for correct answer: {correct_reward}")
        
        # For the first example with complex answer, test with specific format
        if i == 0:
            specific_output = "The speed of the river is v_{R}=4 km/h, and the speed of the boat is v_{B}=10 km/h."
            specific_reward = correctness_reward([specific_output], [example['answer']])[0]
            print(f"Reward for specifically formatted answer: {specific_reward}")
        
        # Test with incorrect answer
        incorrect_output = "The answer is 42"
        incorrect_reward = correctness_reward([incorrect_output], [example['answer']])[0]
        print(f"Reward for incorrect answer: {incorrect_reward}")
        
        # Test step-by-step reward
        step_reward = step_by_step_reward([example['solution']], [example['solution']])[0]
        print(f"Step-by-step reward for reference solution: {step_reward}")
        
        # Test with partial solution
        partial_solution = "\n".join(example['solution'].split("\n")[:5])  # First 5 lines
        partial_reward = step_by_step_reward([partial_solution], [example['solution']])[0]
        print(f"Step-by-step reward for partial solution: {partial_reward}")

if __name__ == "__main__":
    test_reward_functions()