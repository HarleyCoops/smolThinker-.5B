import json

notebook = {
    "nbformat": 4,
    "nbformat_minor": 5,
    "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.9"  # Use a common Python version. Colab usually has 3.9/3.10 available.
        }
    },
    "cells": []
}

def add_code_cell(notebook, code_string):
    notebook["cells"].append({
        "cell_type": "code",
        "execution_count": None,  # Colab will fill this in
        "metadata": {},
        "outputs": [],
        "source": code_string.splitlines(keepends=True)
    })

def add_markdown_cell(notebook, markdown_string):
    notebook["cells"].append({
        "cell_type": "markdown",
        "metadata": {},
        "source": markdown_string.splitlines(keepends=True)
    })


# ---  Notebook Content  ---

add_markdown_cell(notebook, "# OpenR1 Fine-tuning on Qwen 2.5 0.5B")
add_markdown_cell(notebook, "This notebook fine-tunes the Qwen 2.5 0.5B model on the Open-R1-Math-220k dataset for mathematical reasoning.")

# --- Installation ---
add_markdown_cell(notebook, "## 1. Setup and Installations")
add_markdown_cell(notebook, "Install necessary libraries.  **Important:** After running this cell, you might need to **restart the runtime** (Runtime -> Restart Runtime) if prompted by Colab, especially after installing `transformers`. This ensures the newly installed libraries are loaded correctly.")

add_code_cell(notebook,
"""!pip install -q datasets transformers torch scikit-learn peft accelerate bitsandbytes tqdm numpy pandas sentencepiece protobuf tensorboard wandb
""")


# --- Imports ---
add_markdown_cell(notebook, "## 2. Imports")

add_code_cell(notebook,
"""import json
import re
import math
import torch
from datasets import load_dataset
from sklearn.model_selection import train_test_split
from transformers import (
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    TrainerCallback
)
from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training
import wandb
""")

# --- Function Definitions ---
add_markdown_cell(notebook, "## 3. Function Definitions")
add_markdown_cell(notebook, "These functions encapsulate the core logic of the pipeline.")

# Add each function as a separate code cell for better organization and debugging.
add_code_cell(notebook,
"""def load_and_explore_dataset():
    \"\"\"Load and explore the Open-R1-Math-220k dataset.\"\"\"
    print("Loading Open-R1-Math-220k dataset...")
    openr1_dataset = load_dataset("open-r1/OpenR1-Math-220k")

    # Inspect structure
    print("\\nDataset structure:")
    print(openr1_dataset)

    # Examine a sample
    print("\\nSample example:")
    print(openr1_dataset['train'][0])

    # Check dataset size
    print(f"\\nTraining examples: {len(openr1_dataset['train'])}")
    if 'validation' in openr1_dataset:
        print(f"Validation examples: {len(openr1_dataset['validation'])}")
    else:
        print("No validation split found in the dataset.")

    return openr1_dataset""")

add_code_cell(notebook,
"""def preprocess_dataset(dataset, split="train"):
    \"\"\"
    Extract problems, solutions, and answers from the dataset.

    Args:
        dataset: The Open-R1-Math-220k dataset
        split: The dataset split to process

    Returns:
        Tuple of (problems, solutions, answers)
    \"\"\"
    problems = []
    solutions = []
    answers = []

    print(f"Processing {split} split...")

    for example in dataset[split]:
        problem = example["problem"]
        solution = example["solution"]
        answer = example["answer"]

        problems.append(problem)
        solutions.append(solution)
        answers.append(answer)

    print(f"Processed {len(problems)} examples from {split} split.")
    return problems, solutions, answers""")

add_code_cell(notebook,
"""def create_train_val_split(dataset):
    \"\"\"
    Create training and validation splits from the dataset.

    Args:
        dataset: The Open-R1-Math-220k dataset

    Returns:
        Tuple of (train_problems, train_solutions, train_answers, 
                 val_problems, val_solutions, val_answers)
    \"\"\"
    # Process training data
    train_problems, train_solutions, train_answers = preprocess_dataset(dataset, "train")

    # Process validation data (if available)
    if 'validation' in dataset:
        val_problems, val_solutions, val_answers = preprocess_dataset(dataset, "validation")
    else:
        # Create a validation split if not available
        print("Creating validation split from training data...")
        train_problems, val_problems, train_solutions, val_solutions, train_answers, val_answers = train_test_split(
            train_problems, train_solutions, train_answers, test_size=0.1, random_state=42
        )
        print(f"Split into {len(train_problems)} training and {len(val_problems)} validation examples.")

    return (
        train_problems, train_solutions, train_answers,
        val_problems, val_solutions, val_answers
    )""")

add_code_cell(notebook,
"""def extract_openr1_answer(generated_text):
    \"\"\"
    Extract the answer from the generated text.

    Args:
        generated_text: The text generated by the model

    Returns:
        The extracted answer
    \"\"\"
    # Check for "The answer is:" format
    answer_pattern = r'The answer is:?\\s*(.*?)(?:\\n|$)'
    match = re.search(answer_pattern, generated_text, re.IGNORECASE)
    if match:
        return match.group(1).strip()

    # Check if the text already contains a formatted answer like v_{R}=4
    var_pattern = r'v_\\{[^}]+\\}\\s*=\\s*\\d+'
    if re.search(var_pattern, generated_text):
        return generated_text.strip()

    # Try to extract from LaTeX format (common in math problems)
    latex_patterns = [
        r'=\\s*(\\d+(?:\\.\\d+)?)',  # Match = number (possibly with decimal)
        r'(\\d+(?:\\.\\d+)?)\\s*$'   # Match number at the end of text
    ]

    for pattern in latex_patterns:
        match = re.search(pattern, generated_text)
        if match:
            return match.group(1).strip()

    # Try to extract from XML tags if present
    match = re.search(r"<answer>\\s*(.*?)\\s*</answer>", generated_text, re.DOTALL)
    if match:
        return match.group(1).strip()

    # If no pattern matches, return the last line as a fallback
    lines = generated_text.strip().split('\\n')
    return lines[-1].strip()""")

add_code_cell(notebook,
"""def normalize_answer(answer):
    \"\"\"
    Normalize an answer for comparison.

    Args:
        answer: The answer to normalize

    Returns:
        The normalized answer
    \"\"\"
    # For simple numeric answers
    if re.match(r'^\\s*\\d+(?:\\.\\d+)?\\s*$', answer):
        return answer.strip()

    # For complex answers with variables, extract all numbers
    numbers = re.findall(r'=\\s*(\\d+(?:\\.\\d+)?)', answer)
    if numbers:
        return ','.join(numbers)

    # Remove LaTeX formatting as a fallback
    answer = re.sub(r'\\\\mathrm\\{[^}]*\\}', '', answer)
    answer = re.sub(r'\\\\', '', answer)
    answer = re.sub(r'\\{|\\}', '', answer)

    return answer.strip()""")

add_code_cell(notebook,
"""def compare_answers(generated, reference):
    \"\"\"
    Compare a generated answer with a reference answer.

    Args:
        generated: The answer generated by the model
        reference: The reference answer

    Returns:
        True if the answers match, False otherwise
    \"\"\"
    generated_norm = normalize_answer(generated)
    reference_norm = normalize_answer(reference)

    # If both contain multiple numbers (like v_R=4, v_B=10)
    gen_numbers = re.findall(r'\\d+(?:\\.\\d+)?', generated_norm)
    ref_numbers = re.findall(r'\\d+(?:\\.\\d+)?', reference_norm)

    if len(gen_numbers) > 1 and len(ref_numbers) > 1:
        # Check if all numbers match (order-independent)
        return sorted(gen_numbers) == sorted(ref_numbers)

    # Try numeric comparison for single numbers
    try:
        gen_float = float(generated_norm)
        ref_float = float(reference_norm)
        return math.isclose(gen_float, ref_float, rel_tol=1e-8)
    except (ValueError, TypeError):
        # Fall back to string comparison
        return generated_norm == reference_norm""")

add_code_cell(notebook,
"""def correctness_reward_func(generated_texts, reference_answers):
    \"\"\"
    Calculate correctness rewards for generated answers.

    Args:
        generated_texts: List of texts generated by the model
        reference_answers: List of reference answers

    Returns:
        List of reward scores (1 for correct, 0 for incorrect)
    \"\"\"
    rewards = []
    for gen_text, ref_answer in zip(generated_texts, reference_answers):
        extracted_answer = extract_openr1_answer(gen_text)
        if compare_answers(extracted_answer, ref_answer):
            rewards.append(1.0)
        else:
            rewards.append(0.0)
    return rewards""")

add_code_cell(notebook,
"""def step_by_step_reward_func(generated_texts, reference_solutions):
    \"\"\"
    Calculate rewards for step-by-step reasoning.

    Args:
        generated_texts: List of texts generated by the model
        reference_solutions: List of reference solutions

    Returns:
        List of reward scores (0-1 based on similarity to reference solution)
    \"\"\"
    rewards = []
    for gen_text, ref_solution in zip(generated_texts, reference_solutions):
        # Count the number of steps (lines) in the solution
        gen_steps = len([line for line in gen_text.split('\\n') if line.strip()])
        ref_steps = len([line for line in ref_solution.split('\\n') if line.strip()])

        # Calculate a simple ratio of steps
        step_ratio = min(gen_steps / max(1, ref_steps), 1.0)

        # Check if key equations are present
        key_equations = re.findall(r'\\\\begin\\{array\\}.*?\\\\end\\{array\\}', ref_solution, re.DOTALL)
        equation_score = 0.0
        if key_equations:
            for eq in key_equations:
                if eq in gen_text:
                    equation_score += 1.0 / len(key_equations)
        else:
            # If no equations found, look for mathematical expressions
            math_expressions = re.findall(r'\\$.*?\\$', ref_solution)
            if math_expressions:
                for expr in math_expressions:
                    if expr in gen_text:
                        equation_score += 1.0 / len(math_expressions)
            else:
                # If no math expressions found, give full score for this part
                equation_score = 1.0

        # Combine scores (50% for steps, 50% for equations)
        combined_score = 0.5 * step_ratio + 0.5 * equation_score
        rewards.append(combined_score)

    return rewards""")


add_code_cell(notebook,
"""def load_model_and_tokenizer(model_name="Qwen/Qwen2.5-0.5B"):
    \"\"\"
    Load the base model and tokenizer.

    Args:
        model_name: The name of the model to load

    Returns:
        Tuple of (model, tokenizer)
    \"\"\"
    print(f"Loading model and tokenizer: {model_name}")
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(
        model_name,
        torch_dtype=torch.float16,
        device_map="auto"
    )

    # Configure the tokenizer for the model
    tokenizer.pad_token = tokenizer.eos_token

    return model, tokenizer""")

add_code_cell(notebook,
"""def prepare_model_for_training(model):
    \"\"\"
    Prepare the model for training with LoRA.

    Args:
        model: The base model

    Returns:
        The model prepared for training
    \"\"\"
    # Configure LoRA
    lora_config = LoraConfig(
        r=16,
        lora_alpha=32,
        target_modules=["q_proj", "k_proj", "v_proj", "o_proj"],
        lora_dropout=0.05,
        bias="none",
        task_type="CAUSAL_LM"
    )

    # Prepare model for k-bit training if using quantization
    model = prepare_model_for_kbit_training(model)

    # Get PEFT model
    model = get_peft_model(model, lora_config)

    return model""")

add_code_cell(notebook,
"""def setup_training_arguments(output_dir="./results"):
    \"\"\"
    Set up training arguments.

    Args:
        output_dir: Directory to save the model

    Returns:
        TrainingArguments object
    \"\"\"
    return TrainingArguments(
        output_dir=output_dir,
        num_train_epochs=1,  # Start with one epoch
        per_device_train_batch_size=1,
        gradient_accumulation_steps=4,  # Adjust based on GPU memory
        learning_rate=1e-6,  # Lower learning rate for continued fine-tuning
        weight_decay=0.01,
        logging_dir="./logs",
        logging_steps=10,
        evaluation_strategy="steps",
        eval_steps=100,
        save_strategy="steps",
        save_steps=100,
        load_best_model_at_end=True,
        metric_for_best_model="eval_loss",
        fp16=True,  # Use mixed precision training
        report_to="wandb",  # Enable Weights & Biases reporting
    )""")

add_code_cell(notebook,
"""def format_prompt(problem):
    \"\"\"
    Format the prompt for the model.

    Args:
        problem: The mathematical problem to format

    Returns:
        The formatted prompt
    \"\"\"
    return f\"\"\"Solve the following mathematical problem step by step. After solving, clearly state the final answer.

Problem: {problem}

Solution:\"\"\"""")

add_code_cell(notebook,
"""def prepare_training_data(problems, solutions, answers, tokenizer):
    \"\"\"
    Prepare the training data for the model.

    Args:
        problems: List of problems
        solutions: List of solutions
        answers: List of answers
        tokenizer: The tokenizer

    Returns:
        The prepared training data
    \"\"\"
    training_data = []

    for problem, solution, answer in zip(problems, solutions, answers):
        prompt = format_prompt(problem)

        # Format the full solution with a clear answer at the end
        if "The answer is" not in solution and "the answer is" not in solution.lower():
            full_solution = solution + f"\\n\\nThe answer is: {answer}"
        else:
            full_solution = solution

        # Combine prompt and solution for causal language modeling
        full_text = prompt + " " + full_solution

        # Tokenize the full text
        tokenized = tokenizer(full_text, return_tensors="pt", padding=False, truncation=True, max_length=1024)

        # Create input_ids and attention_mask
        input_ids = tokenized.input_ids[0]
        attention_mask = tokenized.attention_mask[0]

        # For causal LM, labels are the same as input_ids
        labels = input_ids.clone()

        # Set prompt tokens to -100 so they're not included in loss calculation
        prompt_tokens = tokenizer(prompt, return_tensors="pt").input_ids[0]
        prompt_length = len(prompt_tokens)
        labels[:prompt_length] = -100

        training_data.append({
            "input_ids": input_ids,
            "attention_mask": attention_mask,
            "labels": labels
        })

    return training_data""")

add_code_cell(notebook,
"""def evaluate_model(model, tokenizer, problems, answers, num_examples=10):
    \"\"\"
    Evaluate the model on test examples.

    Args:
        model: The trained model
        tokenizer: The tokenizer
        problems: List of problems
        answers: List of answers
        num_examples: Number of examples to evaluate

    Returns:
        The accuracy of the model
    \"\"\"
    correct = 0
    model.eval()

    for i in range(min(num_examples, len(problems))):
        prompt = format_prompt(problems[i])
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = generated_text[len(prompt):]  # Remove the prompt

        extracted_answer = extract_openr1_answer(generated_text)

        # Use the improved answer comparison logic
        if compare_answers(extracted_answer, answers[i]):
            correct += 1

        print(f"\\nQuestion: {problems[i]}")
        print(f"Generated: {generated_text}")
        print(f"Extracted answer: {extracted_answer}")
        print(f"Normalized extracted: {normalize_answer(extracted_answer)}")
        print(f"Reference answer: {answers[i]}")
        print(f"Normalized reference: {normalize_answer(answers[i])}")
        print(f"Correct: {compare_answers(extracted_answer, answers[i])}")

    accuracy = correct / num_examples
    print(f"\\nAccuracy: {accuracy:.2f}")
    return accuracy""")

add_code_cell(notebook,
"""def evaluate_step_by_step_reasoning(model, tokenizer, problems, solutions, num_examples=5):
    \"\"\"
    Evaluate the model's step-by-step reasoning quality.

    Args:
        model: The trained model
        tokenizer: The tokenizer
        problems: List of problems
        solutions: List of reference solutions
        num_examples: Number of examples to evaluate

    Returns:
        The average step-by-step reasoning score
    \"\"\"
    model.eval()
    reasoning_scores = []

    for i in range(min(num_examples, len(problems))):
        prompt = format_prompt(problems[i])
        inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_length=512,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )

        generated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)
        generated_text = generated_text[len(prompt):]  # Remove the prompt

        # Calculate step-by-step reasoning score
        reasoning_score = step_by_step_reward_func([generated_text], [solutions[i]])[0]
        reasoning_scores.append(reasoning_score)

        print(f"\\nProblem {i+1}:")
        print(f"Problem: {problems[i][:100]}...")  # Show first 100 chars
        print(f"Generated solution (first 100 chars): {generated_text[:100]}...")
        print(f"Step-by-step reasoning score: {reasoning_score:.2f}")

    avg_score = sum(reasoning_scores) / len(reasoning_scores) if reasoning_scores else 0
    print(f"\\nAverage step-by-step reasoning score: {avg_score:.2f}")
    return avg_score""")

add_code_cell(notebook,
"""class WandbCallback(TrainerCallback):
    \"\"\"Custom callback for logging metrics to wandb during training.\"\"\"

    def __init__(self, trainer):
        super().__init__()
        self.trainer = trainer
        self.training_step = 0

    def on_log(self, args, state, control, logs=None, **kwargs):
        \"\"\"Log metrics to wandb on each logging step.\"\"\"
        if logs is None:
            return

        try:

            if wandb.run is not None:
                # Log training metrics
                for key, value in logs.items():
                    if isinstance(value, (int, float)):
                        wandb.log({key: value}, step=self.training_step)

                self.training_step += 1
        except ImportError:
            pass
        except Exception as e:
            print(f"Error logging metrics to wandb: {str(e)}")""")

# --- Main Execution ---
add_markdown_cell(notebook, "## 4. Main Execution")
add_markdown_cell(notebook, "This section brings all the components together to run the fine-tuning process.")


add_code_cell(notebook,
"""def main():
    # Step 1: Load and explore dataset
    dataset = load_and_explore_dataset()

    # Step 2: Preprocess dataset
    train_problems, train_solutions, train_answers, val_problems, val_solutions, val_answers = create_train_val_split(dataset)

    # Step 3: Not testing answer extraction in the notebook itself

    # Step 4: Load model and tokenizer
    model, tokenizer = load_model_and_tokenizer()

    # Prepare model for training
    model = prepare_model_for_training(model)

    # Initialize wandb
    wandb_initialized = False
    try:

        wandb.init(
            project="openr1-math-finetuning",
            name=f"qwen-0.5B-openr1-math",
            config={
                "model_name": "Qwen/Qwen2.5-0.5B",
                "dataset": "open-r1/OpenR1-Math-220k",
                "learning_rate": 1e-6,
                "batch_size": 1,
                "gradient_accumulation_steps": 4,
                "epochs": 1,
            }
        )
        wandb_initialized = True
        print("Weights & Biases initialized successfully.")
    except ImportError:
        print("Weights & Biases not installed. Continuing without wandb tracking.")
    except Exception as e:
        print(f"Error initializing Weights & Biases: {str(e)}. Continuing without wandb tracking.")

    # Set up training arguments
    training_args = setup_training_arguments()

    # Prepare training data
    train_data = prepare_training_data(train_problems, train_solutions, train_answers, tokenizer)
    val_data = prepare_training_data(val_problems, val_solutions, val_answers, tokenizer)

    # Set up trainer
    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_data,
        eval_dataset=val_data,
    )

    # Add wandb callback if wandb is initialized
    if wandb_initialized:
        try:
            trainer.add_callback(WandbCallback(trainer))
            print("Added Weights & Biases callback for metric logging.")
        except Exception as e:
            print(f"Error adding Weights & Biases callback: {str(e)}")

    # Start training
    print("\\nStarting training...")
    trainer.train()

    # Save the model
    print("\\nSaving model...")
    trainer.save_model("./openr1_finetuned_model")

    # Evaluate on validation set
    print("\\nEvaluating answer correctness...")
    accuracy = evaluate_model(model, tokenizer, val_problems, val_answers, num_examples=20)

    # Evaluate step-by-step reasoning
    print("\\nEvaluating step-by-step reasoning...")
    reasoning_score = evaluate_step_by_step_reasoning(model, tokenizer, val_problems, val_solutions, num_examples=5)

    # Log final metrics to wandb
    if wandb_initialized:
        try:

            wandb.log({
                "final_accuracy": accuracy,
                "final_reasoning_score": reasoning_score
            })
            wandb.finish()
            print("Weights & Biases run completed successfully.")
        except Exception as e:
            print(f"Error logging final metrics to Weights & Biases: {str(e)}")


if __name__ == "__main__":
    main()""")


# --- Call Main ---
add_markdown_cell(notebook, "## 5. Run the Pipeline")
add_markdown_cell(notebook, "Call the `main()` function to execute the entire pipeline.")
add_code_cell(notebook, "main()")  # This line *calls* the main function


# ---  Create the .ipynb file ---
with open("openr1_finetuning_pipeline.ipynb", "w") as f:
    json.dump(notebook, f, indent=2)

print("Notebook 'openr1_finetuning_pipeline.ipynb' created successfully.")